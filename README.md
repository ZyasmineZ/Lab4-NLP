# Lab4-NLP

## Objective

Familiarize with NLP language models using PyTorch.

## Summary

### Part 1: Classification and Regression

- **Data Collection**: Scraped Arabic text data from various websites and assigned relevance scores.
- **Preprocessing**: Applied tokenization, stemming, lemmatization, stop words removal, and discretization.
- **Model Training**: Trained RNN, Bidirectional RNN, GRU, and LSTM models with hyper-parameter tuning.
- **Evaluation**: Evaluated models using standard metrics and BLEU score.

### Part 2: Transformer (Text Generation)

- **Setup**: Installed `pytorch-transformers` and loaded the GPT-2 model.
- **Fine-Tuning and Text Generation**: Fine-tuned GPT-2 on a custom dataset and generated text based on given sentences.

### Part 3: BERT

- **Model Setup**: Used the pre-trained `bert-base-uncased` model.
- **Model Training**: Fine-tuned the BERT model with custom data and optimized hyper-parameters.
- **Evaluation**: Assessed model performance using accuracy, loss, F1 score, BLEU score, and BERTScore.
- **Conclusion**: Provided insights on the effectiveness of the pre-trained BERT model.

## Learning Outcomes

Gained experience in data collection, preprocessing, training RNN, GRU, and LSTM models, fine-tuning transformers (GPT-2, BERT), and evaluating models using various metrics.
